{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#ENVIROMENTAL SETUP\n!pip install -U -q PyDrive\nfrom pydrive.auth import GoogleAuth\nfrom pydrive.drive import GoogleDrive\nfrom google.colab import auth\nfrom oauth2client.client import GoogleCredentials\n# Authenticate and create the PyDrive client.\nauth.authenticate_user()\ngauth = GoogleAuth()\ngauth.credentials = GoogleCredentials.get_application_default()\ndrive = GoogleDrive(gauth)\n\n#DATA FILES IMPORT RESOLVE\n\n#https://drive.google.com/file/d/1gIA9AC8wNx2kdepuP2Y7K5J7LnihRD1V/view?usp=sharing\ndownloaded = drive.CreateFile({'id':'1gIA9AC8wNx2kdepuP2Y7K5J7LnihRD1V'}) \ndownloaded.GetContentFile('Ratings.csv') \n#https://drive.google.com/file/d/11duMozcad56OxTZtGrW3nY2Ir0hKU8ul/view?usp=sharing\ndownloaded = drive.CreateFile({'id':'11duMozcad56OxTZtGrW3nY2Ir0hKU8ul'}) \ndownloaded.GetContentFile('Teleplay.csv')\n#https://drive.google.com/file/d/1TFQWYDQYwBf3cEtVsU6BtUnUZrwGOreH/view?usp=sharing\ndownloaded = drive.CreateFile({'id':'1TFQWYDQYwBf3cEtVsU6BtUnUZrwGOreH'}) \ndownloaded.GetContentFile('New_Teleplay.csv')\n\n#LOADING THE DATA\nimport pandas as pd\ndf=pd.read_csv('Teleplay.csv')\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nv = TfidfVectorizer()\ndf=df.fillna(\"\")\nx = v.fit_transform(df['genre'])\n\n#DEFINE THE COUNT MATRIX\n\n#count_matrix = cv.fit_transform(x)\ncount_matrix = x\n\n#DEFINE THE COSINE MATRIX\ncosine_sim = cosine_similarity(count_matrix)\n\n#TITLE INDEX\ndef get_title_from_index(index):\n    return df[df.index == index][\"name\"].values[0]\ndef get_index_from_title(title):\n    return df[df.name == title].index[0]\n#///////////////\ndf.head()\n\n\n\n#READ THE DATA FROM THE DATA FILES\n\ndfr=pd.read_csv(\"Ratings.csv\")\ndfr.head()\n\n#DATA IDENTIFICATION\nuser=dfr[dfr[\"user_id\"]==53698]\nrequired=user['rating'].max()\nuser_likes=user[user['rating']==required]\n\n#DATA MERGE\nuser_likes=pd.merge(user_likes,df[['teleplay_id','name','rating']],how='inner',left_on='teleplay_id',right_on='teleplay_id')\nuser_likes = user_likes.rename(columns = {'rating_x':'user rating','rating_y':'general rating'})\nuser_likes\n\n\n#FURTHER RUNS\nmovies={}\nfor n in df['name']:\n  movies[n]=0\n\n\n#SORTING \nfor n in user_likes['name']:\n  movie_user_likes = n\n  movie_index = get_index_from_title(movie_user_likes)\n  similar_movies = list(enumerate(cosine_sim[movie_index]))\n  sorted_similar_movies = sorted(similar_movies,key=lambda x:x[1],reverse=True)[1:]\n  i=0\n  for element in sorted_similar_movies:\n      movies[get_title_from_index(element[0])]+=1\n      print(get_title_from_index(element[0]),end = \" \")\n      i=i+1\n      if i>5:\n          break\n\n#RECOMMENDING OPTIONS\nrecommended = sorted(movies.items(), key=lambda x:x[1],reverse=True)[:100]\nprint(\"The top 5 movies recommended are : \")\ni=0\nfor l,k in recommended:\n  if l not in list(user_likes['name']):\n      print(l)\n      i+=1\n  if i==5:\n    break\n    \n#Collaborative Filtering\ndfr['rating'].replace(-1,0,inplace=True)\ndfc=pd.merge(dfr,df[['teleplay_id','name']],how='inner',left_on='teleplay_id',right_on='teleplay_id')\ndfc=dfc.drop('teleplay_id',axis=1)\ndfc.head()\n\n#RELATIVE INFLUENCE\nmovie_matrix_UII = dfc.pivot_table(index='user_id', columns='name', values='rating')\nmovie_matrix_UII.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install -U -q PyDrive\nfrom pydrive.auth import GoogleAuth\nfrom pydrive.drive import GoogleDrive\nfrom google.colab import auth\nfrom oauth2client.client import GoogleCredentials\n# Authenticate and create the PyDrive client.\nauth.authenticate_user()\ngauth = GoogleAuth()\ngauth.credentials = GoogleCredentials.get_application_default()\ndrive = GoogleDrive(gauth)","metadata":{"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-b7ed32bca166>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip install -U -q PyDrive\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-2-b7ed32bca166>, line 1)","output_type":"error"}]},{"cell_type":"code","source":"#https://drive.google.com/file/d/1gIA9AC8wNx2kdepuP2Y7K5J7LnihRD1V/view?usp=sharing\ndownloaded = drive.CreateFile({'id':'1gIA9AC8wNx2kdepuP2Y7K5J7LnihRD1V'}) \ndownloaded.GetContentFile('Ratings.csv') \n#https://drive.google.com/file/d/11duMozcad56OxTZtGrW3nY2Ir0hKU8ul/view?usp=sharing\ndownloaded = drive.CreateFile({'id':'11duMozcad56OxTZtGrW3nY2Ir0hKU8ul'}) \ndownloaded.GetContentFile('Teleplay.csv')\n#https://drive.google.com/file/d/1TFQWYDQYwBf3cEtVsU6BtUnUZrwGOreH/view?usp=sharing\ndownloaded = drive.CreateFile({'id':'1TFQWYDQYwBf3cEtVsU6BtUnUZrwGOreH'}) \ndownloaded.GetContentFile('New_Teleplay.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ntp=pd.read_csv(\"Teleplay.csv\")\ntp=tp.drop('name',axis=1)\ntp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tp['type'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rt=pd.read_csv('Ratings.csv')\nrt.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rt.loc[rt['rating']==-1,'rating']= None\nrt.head()","metadata":{"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-4f2a49509973>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'rt' is not defined"],"ename":"NameError","evalue":"name 'rt' is not defined","output_type":"error"}]},{"cell_type":"code","source":"tp.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gen=tp['genre'].fillna(\"\")\ntp=tp.drop('genre',axis=1)\ntp['genre']=gen\ntp.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tp['type'].value_counts())\ntyp=tp['type'].fillna(tp['type'].value_counts().idxmax())\ntp=tp.drop('type',axis=1)\ntp['type']=typ\ntp.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tp[tp['episodes']=='Unknown']=None\ng=tp['episodes'].dropna()\ntp=tp.drop('episodes',axis=1)\ntp['episodes']=g\ntp.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tp=tp.dropna()\ntp.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Handling Outliers","metadata":{}},{"cell_type":"code","source":"outliers=[]\ndef detect_outliers(data):\n    \n    threshold=3\n    mean = np.mean(data)\n    std =np.std(data)\n    \n    \n    for i in data:\n        z_score= (i - mean)/std \n        if np.abs(z_score) > threshold:\n            outliers.append(i)\n    return outliers\n\nlen(outlier_pt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outlier_pt=detect_outliers(tp['members'].astype(int))\ntp.drop(tp[tp['members'].isin(outlier_pt)].index,inplace=True)\n\noutlier_pt=detect_outliers(tp['episodes'].astype(int))\ntp.drop(tp[tp['episodes'].isin(outlier_pt)].index,inplace=True)\ntp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ntyp_label=le.fit_transform(tp['type'])\ntp=tp.drop('type',axis=1)\ntp['type']=typ_label\ntp.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tp=tp.reset_index(drop=True)\ntp.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n  \nonehotencoder = OneHotEncoder()\nr=pd.DataFrame(tp['type'])\n#data = np.array(columnTransformer.fit_transform(r), dtype = np.str)\nrated_dummies = pd.get_dummies(r.type)\ntype_names={}\nfor i in range(6):\n  type_names[i]=le.inverse_transform([i])[0]\nrated_dummies=rated_dummies.rename(columns=type_names)\nrated_dummies.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for r in rated_dummies.columns:\n  tp[r]=rated_dummies[r]\ntp.head()","metadata":{"trusted":true},"execution_count":4,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-adee422f3cb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrated_dummies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mtp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrated_dummies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'rated_dummies' is not defined"],"ename":"NameError","evalue":"name 'rated_dummies' is not defined","output_type":"error"}]},{"cell_type":"code","source":"tp=tp.drop('type',axis=1)\ntp.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Add average rating from the users to tp dataframe","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"rt=rt.dropna().reset_index(drop=True)\nrt.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_rt=pd.DataFrame(rt.groupby('teleplay_id').mean()['rating'])\ndf=pd.merge(tp, avg_rt, how='inner', left_on = 'teleplay_id', right_on = 'teleplay_id')\ndf=df.rename(columns = {\"rating_x\": \"rating\",\"rating_y\":\"average rating\"})\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nv = TfidfVectorizer()\nx = v.fit_transform(df['genre'])\ngen_idf=pd.DataFrame(x.toarray())\ngen_idf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\n# create a scaler object\nscaler = MinMaxScaler()\n# fit and transform the data\ndf_norm = pd.DataFrame(scaler.fit_transform(df[['members','episodes']]), columns=['members','episodes'])\n\nfor r in rated_dummies.columns:\n  df_norm[r]=rated_dummies[r]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_norm=df_norm.drop('Music',axis=1)\n","metadata":{"trusted":true},"execution_count":5,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-91c02f325a23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Music'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'df_norm' is not defined"],"ename":"NameError","evalue":"name 'df_norm' is not defined","output_type":"error"}]},{"cell_type":"code","source":"for i in range(47):\n  df_norm[i]=gen_idf[i]\ndf_norm.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=df['rating']\nX=df_norm\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nregressor=LinearRegression()\nregressor.fit(X_train,y_train)\n\ny_pred=regressor.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\nrms = mean_squared_error(y_test, y_pred, squared=False)\nrms","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regressor=LinearRegression()\nregressor.fit(X,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df=pd.read_csv(\"New_Teleplay.csv\")\ntest_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nv = TfidfVectorizer()\ngen_test=test_df['genre'].fillna(\"\")\ntest_df=test_df.drop('genre',axis=1)\ntest_df['genre']=gen_test\nx = v.fit_transform(test_df['genre'])\ngen_idf_test=pd.DataFrame(x.toarray())\ngen_idf_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_df['type'].value_counts())\ntyp=test_df['type'].fillna(test_df['type'].value_counts().idxmax())\ntest_df=test_df.drop('type',axis=1)\ntest_df['type']=typ\ntest_df.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"One Hot Encoding","metadata":{}},{"cell_type":"code","source":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ntyp_label=le.fit_transform(test_df['type'])\ntest_df=test_df.drop('type',axis=1)\ntest_df['type']=typ_label\ntest_df.head()\n\n# One Hot Encoding\n\nfrom sklearn.preprocessing import OneHotEncoder\nonehotencoder = OneHotEncoder()\nr=pd.DataFrame(test_df['type'])\n#data = np.array(columnTransformer.fit_transform(r), dtype = np.str)\nrated_dummies = pd.get_dummies(r.type)\ntype_names={}\nfor i in range(6):\n  type_names[i]=le.inverse_transform([i])[0]\nrated_dummies=rated_dummies.rename(columns=type_names)\nrated_dummies.head()","metadata":{"trusted":true},"execution_count":6,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-1eab98ecaa66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtyp_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp_label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'test_df' is not defined"],"ename":"NameError","evalue":"name 'test_df' is not defined","output_type":"error"}]},{"cell_type":"code","source":"test_df.loc[test_df['episodes']=='Unknown','episodes']= None\ntest_df=test_df.drop('rating',axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df=test_df.fillna(test_df.mean())\nfrom sklearn.preprocessing import MinMaxScaler\n\n# create a scaler object\nscaler = MinMaxScaler()\n# fit and transform the data\ndf_norm_test = pd.DataFrame(scaler.fit_transform(test_df[['members','episodes']]), columns=['members','episodes'])\n\nfor r in rated_dummies.columns:\n  df_norm_test[r]=rated_dummies[r]\ndf_norm_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_norm_test=df_norm_test.drop('Music',axis=1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(47):\n  df_norm_test[i]=gen_idf_test[i]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_norm_test=df_norm_test.fillna(df_norm_test.mean())\ny_ans=regressor.predict(df_norm_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ans_df=pd.read_csv('New_Teleplay.csv')\nans_df.drop('rating',axis=1)\nans_df['rating']=y_ans\nans_df.to_csv(\"Final_rating.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"NEURAL NETWORK","metadata":{}},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error \nfrom matplotlib import pyplot as plt\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport warnings ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"MODEL FIXING\n","metadata":{}},{"cell_type":"code","source":"NN_model = Sequential()\n\n# The Input Layer :\nNN_model.add(Dense(256, kernel_initializer='normal',input_dim = X.shape[1], activation='relu'))\n\n# The Hidden Layers :\nNN_model.add(Dense(512, kernel_initializer='normal',activation='relu'))\nNN_model.add(Dense(512, kernel_initializer='normal',activation='relu'))\nNN_model.add(Dense(512, kernel_initializer='normal',activation='relu'))\n\n# The Output Layer :\nNN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n\n# Compile the network :\nNN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\nNN_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CHECKPOINT","metadata":{}},{"cell_type":"code","source":"checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \ncheckpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\ncallbacks_list = [checkpoint]\nNN_model.fit(X, y, epochs=300, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wights_file = '/content/Weights-198--0.62641.hdf5' # choose the best checkpoint \nNN_model.load_weights(wights_file) # load it\nNN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_submission(prediction, sub_name):\n  my_submission = pd.DataFrame({'Id':pd.read_csv('test.csv').Id,'SalePrice':prediction})\n  my_submission.to_csv('{}.csv'.format(sub_name),index=False)\n  print('A submission file has been made')\n\npredictions = NN_model.predict(df_norm_test)\n#make_submission(predictions[:,0],'submission(NN).csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ans_df=pd.read_csv('New_Teleplay.csv')\nans_df.drop('rating',axis=1)\nans_df['rating']=predictions\nans_df.to_csv(\"Final_rating_NN.csv\")","metadata":{},"execution_count":null,"outputs":[]}]}